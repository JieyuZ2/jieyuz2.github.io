(\* denotes equal contribution, = denotes student I mentor)

### Preprint
- [Adaptive In-conversation Team Building for Language Model Agents](https://arxiv.org/abs/2405.19425)
<br>Linxin Song, Jiale Liu, **Jieyu Zhang**, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, Chi Wang.

### 2024
- [Task Me Anything](https://arxiv.org/abs/2406.11775)
<br>**Jieyu Zhang**, Weikai Huang\*, Zixian Ma\*, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, Ranjay Krishna.
<br>*NeurIPS 2024*
- [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794)
<br>59 authors.
<br>*NeurIPS 2024*
- [xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://arxiv.org/abs/2408.08872)
<br>Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, Shrikant Kendre, **Jieyu Zhang**, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu.
<br>*ECCV EVAL-FoMo Workshop 2024*
- [m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks](https://arxiv.org/abs/2403.11085)
<br>Zixian Ma, Weikai Huang, **Jieyu Zhang**, Tanmay Gupta, Ranjay Krishna.
<br>*ECCV 2024*
- [Offline Training of Language Model Agents with Functions as Learnable Weights](https://arxiv.org/abs/2402.11359)
<br>Shaokun Zhang\*, **Jieyu Zhang\***, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, Qingyun Wu.
<br>*ICML 2024*
- [SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2307.10635)
<br>Xiaoxuan Wang\*, Ziniu Hu\*, Pan Lu\*, Yanqiao Zhu\*, **Jieyu Zhang**, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang.
<br>*ICML 2024*
<br><a href="https://www.nature.com/articles/d41586-023-03507-3" style="color: red; text-decoration: underline">Nature News Feature</a>
- [Iterated Learning Improves Compositionality in Large Vision-Language Models]()
<br>Chenhao Zheng=, **Jieyu Zhang**, Aniruddha Kembhavi, Ranjay Krishna.
<br>*CVPR 2024*.
- [EcoAssistant: Using LLM Assistant More Affordably and Accurately](https://arxiv.org/abs/2310.03046)
<br>**Jieyu Zhang**, Ranjay Krishna, Ahmed Awadallah, Chi Wang.
<br>*LLM Agents @ ICLR 2024*
- [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155)
<br>Qingyun Wu, Gagan Bansal, **Jieyu Zhang**, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen White, Doug Burger, Chi Wang.
<br><ins>*COLM 2024*</ins> | <ins>*LLM Agents @ ICLR 2024*</ins> <font color=red>Best Paper</font> 
<br><a href="https://github.com/microsoft/autogen" style="color: red; text-decoration: underline">Github 28,000+ Star & 4,000+ Fork</a>
<br><a href="https://www.economist.com/science-and-technology/2024/05/13/todays-ai-models-are-impressive-teams-of-them-will-be-formidable" style="color: red; text-decoration: underline">The Economist article</a>
<br><a href="https://www.forbes.com/sites/joannechen/2024/05/24/the-promise-of-multi-agent-ai/?sh=2c1e4f454d97" style="color: red; text-decoration: underline">The Forbes article</a>

****

### 2023

- [SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality](https://arxiv.org/abs/2306.14610)
<br>Cheng-Yu Hsieh\*, **Jieyu Zhang\***, Zixian Ma, Aniruddha Kembhavi, Ranjay Krishna.
<br>*NeurIPS 2023*
- [DataComp: In Search of the Next Generation of Multimodal Datasets](https://arxiv.org/abs/2304.14108)
<br>34 authors.
<br>*NeurIPS 2023* <font color=red>Oral Presentation</font>
- [On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training](https://arxiv.org/abs/2305.12224)
<br>**Jieyu Zhang\***, Bohan Wang\*, Zhengyu Hu, Pang Wei Koh, Alexander Ratner.
<br>*NeurIPS 2023*
- [Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias](https://arxiv.org/abs/2306.15895)
<br>Yue Yu\*, Yuchen Zhuang\*, **Jieyu Zhang\***, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, Chao Zhang.
<br>*NeurIPS 2023*
- [Characterizing the Impacts of Semi-supervised Learning for Weak Supervision](https://openreview.net/forum?id=Z8TjsPFBSx)
<br>Jeffrey Li, **Jieyu Zhang**, Ludwig Schmidt, Alexander Ratner.
<br>*NeurIPS 2023*
- [Subclass-balancing Contrastive Learning for Long-tailed Recognition](https://arxiv.org/abs/2306.15925)
<br>Chengkai Hou=, **Jieyu Zhang**, Haonan Wang, Tianyi Zhou.
<br>*ICCV 2023*.
- [When to Learn What: Model-Adaptive Data Augmentation Curriculum](https://arxiv.org/abs/2309.04747)
<br>Chengkai Hou=, **Jieyu Zhang**, Tianyi Zhou.
<br>*ICCV 2023*.
- [Leveraging Instance Features for Label Aggregation in Programmatic Weak Supervision](https://arxiv.org/abs/2210.02724)
<br>**Jieyu Zhang\***, Linxin Song=\*, Alexander Ratner.
<br>*AISTATS 2023*

****

### 2022

- [Understanding Programmatic Weak Supervision via Source-aware Influence Function](https://arxiv.org/abs/2205.12879)
<br>**Jieyu Zhang\***, Haonan Wang\*, Cheng-Yu Hsieh, Alexander Ratner.
<br>*NeurIPS 2022*
- [Creating Training Sets via Weak Indirect Supervision](https://arxiv.org/abs/2110.03484)
<br>**Jieyu Zhang**, Bohan Wang, Xiangchen Song, Yujing Wang, Yaming Yang, Jing Bai, Alexander Ratner.
<br>*ICLR 2022*
- [Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming](https://arxiv.org/abs/2203.01382)
<br>Cheng-Yu Hsieh, **Jieyu Zhang**, Alexander Ratner.
<br>*VLDB 2022*

****

### 2021
- [WRENCH: A Comprehensive Benchmark for Weak Supervision](https://arxiv.org/abs/2109.11377)
<br>**Jieyu Zhang**, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, Alexander Ratner.
<br>*NeurIPS 2021* <font color=red>Oral Presentation</font>

****
